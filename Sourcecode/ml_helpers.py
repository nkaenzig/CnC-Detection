"""
Master Thesis
Network Monitoring and Attack Detection

ml_helpers.py
This module contains several helper functions used in the machine learning part of the thesis.

@author: Nicolas Kaenzig, D-ITET, ETH Zurich
"""

import pandas as pd
from matplotlib import cm
from sklearn import preprocessing
from sklearn.decomposition import PCA
from sklearn.metrics import classification_report
from sklearn.manifold import TSNE, LocallyLinearEmbedding, SpectralEmbedding

import sys
from sklearn.preprocessing import OneHotEncoder, LabelEncoder

import matplotlib
# matplotlib.use('Agg')
import matplotlib.pyplot as plt
import os
import pickle
import numpy as np
from datetime import datetime, timedelta, timezone
import time
import csv
from tqdm import tqdm
import random
from joblib import Parallel, delayed


class Logger(object):
    """
    This class can be used to log the outputs generated by print statements (sys.stdout)
    """
    def __init__(self, log_path):
        self.terminal = sys.stdout
        self.log = open(log_path, 'a')

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)

    def flush(self):
        pass

    def close(self):
        self.log.close()


def save_model(model, models_directory, model_name):
    """
    Saves a model in .pickle format

    :param model: Model to be saved.
    :param models_directory: Directory where the .pickle file should be stored
    :param model_name: Filename of the model
    """
    if not os.path.exists(models_directory):
        os.makedirs(models_directory)

    filename = model_name
    with open(os.path.join(models_directory, filename), 'wb') as fp:
        pickle.dump(model, fp)
    print('Model saved to {}'.format(os.path.join(models_directory, filename)))


def load_model(model_path):
    """
    Loads a model stored in .pickle format

    :param model_path: File path of the model
    :return: The loaded model.
    """
    with open(model_path, 'rb') as fp:
        model = pickle.load(fp)
    print('Model loaded from {}'.format(model_path))
    return model


def load_model_and_test(model_path, csv_test_path, selected_features, standardize=True, categorical_feature_mapping=None, one_hot=False, pca_path=None):
    """
    Loads a model stored in .pickle format, runs predictions on a specified testset, and reports performance metrics

    :param model_path: Filepath of the model to be tested
    :param csv_test_path: Path of the testset (in .csv format)
    :param selected_features: List of the selected features/columns to be loaded from the .csv testset
    :param standardize: To enable standardization of the testdata. (See doc of function ml_helpers.load_dataset())
    :param categorical_feature_mapping: Dictionary to map categorical features to numerical values (see doc of function load_dataset())
    :param one_hot: If categorical features are present, set this parameter to True to enable one hot encoding
    :param pca_path: To enable principal component analysis: Path of the PCA that was fitted on the training data
    """
    model = load_model(model_path)

    X, Y, _, _ = load_dataset(csv_test_path, 1, selected_feature_names=selected_features, standardize=standardize,
                              balance=False, categorical_feature_mapping=categorical_feature_mapping, one_hot=one_hot)

    if pca_path:
        print('... PCA')
        with open(pca_path, 'rb') as handle:
            pca = pickle.load(handle)
        X = pca.transform(X)

    print('Performing predictions ...')
    start = time.time()
    Y_predicted = model.predict(X)
    end = time.time()
    print('Took: {}s'.format(end - start))

    print(classification_report(Y, Y_predicted, target_names=['normal', 'malicious']))


def load_model_and_evaluate_multiple(model_path, csv_test_directory, selected_features, standardize=True):
    """
    Loads a model stored in .pickle format, runs predictions on all .csv files in the csv_test_directory directory

    """
    model = load_model(model_path)

    for filename in os.listdir(csv_test_directory):
        if filename.endswith(".csv") and 'labelled' in filename:
            filepath = os.path.join(csv_test_directory, filename)
            print('Making predictions on {} ...'.format(filepath))
            X, Y, _, _ = load_dataset(filepath, 1, selected_feature_names=selected_features, standardize=standardize, balance=False)
            print('Performing predictions ...')
            Y_predicted = model.predict(X)
            print(classification_report(Y, Y_predicted, target_names=['normal', 'malicious']))


def get_train_and_test_filenames(datasets_path):
    """
    Get the filenames of all .csv train and testsets in the specified directory. Ignores files containing 'metadata'.

    :param datasets_path: Directory containing the .csv train and test sets
    :return: Tuple (train-filenames[], test-filenames[])
    """
    train_files = []
    test_files = []

    for f in os.listdir(datasets_path):
        if 'meta' not in f and f.endswith(".csv"):
            if 'train' in f:
                train_files.append(f)
            elif 'test' in f:
                test_files.append(f)

    train_files = sorted(train_files, key=lambda x: int(x.split('-')[-1].split('.')[0]))
    test_files = sorted(test_files, key=lambda x: int(x.split('-')[-1].split('.')[0]))

    return train_files, test_files


### DATASET GENERATION ###

def generate_multiple_sets_by_host(nr_sets, labelled_samples_path, selected_features, subsampling_fraction,
                                   train_hosts_fraction, normal_traffic_fraction, malicious_hosts, suffix,
                                   host_splits=None):
    """
    Runs generate_train_and_test_sets_by_host() (see corresponding doc) several times in parallel

    :param nr_sets: Nr of train/test-sets to be generated
    :param labelled_samples_path: Path of the labelled data containing all samples (in .csv format)
    :param selected_features: List of the selected features/columns
    :param subsampling_fraction: Float in interval [0, 1]. E.g. 0.5 will keep half of the samples. Set to None to deactivate
    :param train_hosts_fraction: see generate_train_and_test_sets_by_host()
    :param normal_traffic_fraction: see generate_train_and_test_sets_by_host()
    :param malicious_hosts: list of malicious hosts
    :param suffix: Suffix for dataset filenames
    :param host_splits: To use predefined host splits for dataset generation
    """
    out_directory = os.path.join(os.path.dirname(labelled_samples_path),
                                 '{}sets-{}-{}-{}-{}-{}'.format(nr_sets, subsampling_fraction, train_hosts_fraction,
                                                                normal_traffic_fraction, suffix,
                                                                str(int(datetime.now().timestamp()))))
    os.makedirs(out_directory)
    print('Saving datasets to directory {} ...'.format(out_directory))

    suffixes = ['-{}-{}'.format(suffix, i) for i in range(nr_sets)]

    # run in in parallel
    if host_splits != None:
        if len(host_splits) > nr_sets:
            host_split = host_splits[:nr_sets]
        print('Using predefined host splits for dataset generation ...')
        results = Parallel(n_jobs=nr_sets)(
            delayed(generate_train_and_test_sets_by_host)(labelled_samples_path, out_directory, selected_features,
                                                          subsampling_fraction, train_hosts_fraction,
                                                          normal_traffic_fraction, malicious_hosts,
                                                          suff, host_split) for suff, host_split in
            zip(suffixes, zip(host_splits[0], host_splits[1])))
    else:
        results = Parallel(n_jobs=10)(
            delayed(generate_train_and_test_sets_by_host)(labelled_samples_path, out_directory, selected_features,
                                                          subsampling_fraction, train_hosts_fraction,
                                                          normal_traffic_fraction,
                                                          malicious_hosts, suff) for suff in suffixes)

    train_mal_hosts = []
    test_mal_hosts = []

    with open(os.path.join(out_directory, "Output.txt"), "w") as text_file_fp:
        for result in results:
            text_file_fp.write(result[0])
            train_mal_hosts.append(result[1])
            test_mal_hosts.append(result[2])

    if host_splits == None:
        with open(os.path.join(out_directory, 'host_train_test_splits.pickle'), 'wb') as fp:
            pickle.dump((train_mal_hosts, test_mal_hosts), fp)


def generate_train_and_test_sets_by_host(labelled_samples_path, output_path, selected_features, subsampling_fraction,
                                         train_hosts_fraction, normal_traffic_fraction, malicious_hosts, suffix,
                                         host_split=None, KDD=False):
    """
    This function splits a labelled dataset (.csv) into two seperate train- and test-sets, with the option to subsample the data and
    to select only a subset of the features (<selected_features>).
    To ensure that the train- and test-sets each contain samples of different Cobalt Strike (CS) sessions, in a first step the
    list <malicious_hosts> holding the IP addresses of all malicious CS listeners is split into two parts, where <train_hosts_fraction>
    of the IPs are randomly assigned to the train-set and the remaining IPs to the test-set.
    After that, malicious samples (=samples containing an IP address from <malicious_hosts>) will be written either to the train-
    or the test-set according to this partitioning of the malicious IPs.
    Meanwhile the first <normal_traffic_fraction>*nr_samples samples will be written to the train-set, and the remaining samples
    will be assigned to the test-set

    :param labelled_samples_path: Path to the .csv file holding all labelled samples
    :param selected_features: List of the selected featured
    :param train_hosts_fraction:    Fraction according to which the <malicious_hosts> list will be splitted, where <train_hosts_fraction>*len(malicious_hosts) of the IPs
                                    will be assigned to the train-set
    :param subsampling_fraction: Fraction to perform random uniform subsampling accross the whole dataset
    :param normal_traffic_fraction: first <normal_traffic_fraction>*nr_samples samples will be written to the train-set, and the remaining samples
                                    will be assigned to the test-set
    :param malicious_hosts: Holding the IP addresses of all malicious CS listeners
    :param KDD: Set to true when using KDD features
    """

    pre_suffix = '-{}-{}-{}'.format(subsampling_fraction, train_hosts_fraction, normal_traffic_fraction)
    suffix = pre_suffix + suffix
    csv_train_path = output_path + '/train_by_hosts' + suffix + '.csv'
    csv_test_path = output_path + '/test_by_hosts' + suffix + '.csv'

    csv_train_meta_path = output_path + '/train_by_hosts' + suffix + '-meta' + '.csv'
    csv_test_meta_path = output_path + '/test_by_hosts' + suffix + '-meta' + '.csv'

    selected_feature_indices = get_indices_of_features_from_csv(labelled_samples_path, selected_features)

    # index the malicious ips and then shuffle them to then randomly subsample two subsets of IPs for the train and testset
    random_host_indices = np.arange(len(malicious_hosts))
    np.random.seed()
    random_host_indices = np.random.permutation(random_host_indices)
    split_idx = int(train_hosts_fraction * len(malicious_hosts))
    train_host_indices = random_host_indices[:split_idx]
    test_host_indices = random_host_indices[split_idx:]

    if host_split:
        train_mal_hosts = host_split[0]
        test_mal_hosts = host_split[1]
    else:
        malicious_hosts = np.array(malicious_hosts)
        train_mal_hosts = set(malicious_hosts[train_host_indices])  # convert to set to make lookups faster
        test_mal_hosts = set(malicious_hosts[test_host_indices])

    nr_train_samples = 0
    nr_mal_train_samples = 0
    nr_test_samples = 0
    nr_mal_test_samples = 0

    tcp_zero_syn_cnt = 0

    with open(labelled_samples_path, 'r') as csv_r, open(csv_train_path, 'w') as csv_train_w, open(csv_test_path,
                                                                                                   'w') as csv_test_w, \
            open(csv_train_meta_path, 'w') as csv_train_meta_w, open(csv_test_meta_path, 'w') as csv_test_meta_w:
        csv_reader = csv.reader(csv_r, delimiter=',', quotechar='#')
        csv_train_writer = csv.writer(csv_train_w, delimiter=',', quotechar='#')
        csv_test_writer = csv.writer(csv_test_w, delimiter=',', quotechar='#')
        csv_train_meta_writer = csv.writer(csv_train_meta_w, delimiter=',', quotechar='#')
        csv_test_meta_writer = csv.writer(csv_test_meta_w, delimiter=',', quotechar='#')

        header = csv_reader.__next__()
        src_idx = header.index('Src IP')
        dst_idx = header.index('Dst IP')
        label_idx = header.index('Label')

        if KDD:
            timestamp_idx = header.index('conn_end_time')
        else:
            timestamp_idx = header.index('Timestamp')
            prot_idx = header.index('Protocol')
            syn_cnt_idx = header.index('SYN Flag Cnt')


        meta_indices = [timestamp_idx, src_idx, dst_idx]

        header = np.array(header)
        csv_train_writer.writerow(header[selected_feature_indices])
        csv_test_writer.writerow(header[selected_feature_indices])
        csv_train_meta_writer.writerow(header[meta_indices])
        csv_test_meta_writer.writerow(header[meta_indices])

        writeTrain = True  # As long as this is True, normal samples will be written to the train-set

        print('Count total number of samples ...')
        nr_samples = sum(1 for line in csv_r)  # count lines in csv
        # reset filepointer to beginning and read one line to set it to line after the header
        csv_r.seek(0)
        csv_r.readline()
        if subsampling_fraction < 1:
            skip = sorted(random.sample(range(nr_samples), nr_samples - int(subsampling_fraction * nr_samples)))
            skip_idx = 0

        for nr, row in enumerate(
                tqdm(csv_reader, total=nr_samples, desc='Parsing the .csv and writing train and test-sets')):
            if subsampling_fraction < 1:
                if skip_idx < len(skip):
                    if nr == skip[skip_idx]:
                        skip_idx += 1
                        continue
                        # print('line {} skipped'.format(nr))

            if not KDD:
                if row[prot_idx] == '6' and row[syn_cnt_idx] == '0':
                    tcp_zero_syn_cnt += 1
                    continue


            # caution: FlowMeter generates times according to local timezone, while malicios_sessions stores UTC timeformat
            UTC_OFFSET = 2
            if KDD:
                dt = datetime.strptime(row[timestamp_idx], '%Y-%m-%dT%H:%M:%S') - timedelta(hours=UTC_OFFSET)
            else:
                dt = datetime.strptime(row[timestamp_idx], '%d/%m/%Y %H:%M:%S') - timedelta(hours=UTC_OFFSET)
            row[timestamp_idx] = dt.replace(tzinfo=timezone.utc).timestamp()

            row = np.array(row)
            if (row[src_idx] in train_mal_hosts or row[dst_idx] in train_mal_hosts) and row[label_idx] == '1':
                csv_train_writer.writerow(row[selected_feature_indices])
                csv_train_meta_writer.writerow(row[meta_indices])
                nr_train_samples += 1
                nr_mal_train_samples += 1
            elif (row[src_idx] in test_mal_hosts or row[dst_idx] in test_mal_hosts) and row[label_idx] == '1':
                csv_test_writer.writerow(row[selected_feature_indices])
                csv_test_meta_writer.writerow(row[meta_indices])
                nr_test_samples += 1
                nr_mal_test_samples += 1
            else:
                if writeTrain:
                    csv_train_writer.writerow(row[selected_feature_indices])
                    csv_train_meta_writer.writerow(row[meta_indices])
                    nr_train_samples += 1
                    if nr > normal_traffic_fraction * nr_samples:
                        writeTrain = False  # from now on write the normal samples to the test-set
                else:
                    csv_test_writer.writerow(row[selected_feature_indices])
                    csv_test_meta_writer.writerow(row[meta_indices])
                    nr_test_samples += 1

    print_out = 'Train- and testsets written to: \n{}\n{}\n\n'.format(csv_train_path, csv_test_path)

    print_out += 'Train-set malicious IPs: {}\n'.format(train_mal_hosts)
    print_out += 'Test-set malicious IPs: {}\n'.format(test_mal_hosts)
    print_out += 'Train-set contains {}% malicious samples ({} out of {})\n'.format(
        nr_mal_train_samples * 100.0 / nr_train_samples, nr_mal_train_samples, nr_train_samples)
    print_out += 'Test-set contains {}% malicious samples ({} out of {})\n'.format(
        nr_mal_test_samples * 100.0 / nr_test_samples, nr_mal_test_samples, nr_test_samples)
    print_out += '{} ({}%) TCP-Flows skipped due to zero SYN count\n\n'.format(tcp_zero_syn_cnt,
                                                                               tcp_zero_syn_cnt * 100.0 / nr_samples)

    print(print_out)

    return (print_out, train_mal_hosts, test_mal_hosts)


def generate_train_and_test_sets(labelled_samples_path, output_directory, selected_features, train_fraction,
                                 subsampling_fraction, suffix):
    """
    Splits the .csv file in labelled_samples_path into two seperate train & test set .csvs

    :param output_directory: Directory to store the generated train & test sets
    :param selected_features: List of selected feature names
    :param train_fraction: To specifiy size of the train/test sets. E.g. 0.5 for equal size
    :param subsampling_fraction: To specify a fraction for subsampling. Set to None to keep all data.
    :param suffix: Suffix for dataset filenames
    :return: (csv_train_path, csv_test_path)
    """

    tcp_zero_syn_cnt = 0

    csv_train_path = output_directory + '/train_' + suffix + '.csv'
    csv_test_path = output_directory + '/test_' + suffix + '.csv'

    with open(labelled_samples_path, 'r') as csv_r, open(csv_train_path, 'w') as csv_train_w, open(csv_test_path,
                                                                                                   'w') as csv_test_w:
        csv_reader = csv.reader(csv_r, delimiter=',', quotechar='#')
        csv_train_writer = csv.writer(csv_train_w, delimiter=',', quotechar='#')
        csv_test_writer = csv.writer(csv_test_w, delimiter=',', quotechar='#')

        header = csv_reader.__next__()
        prot_idx = header.index('Protocol')
        syn_cnt_idx = header.index('SYN Flag Cnt')

        if selected_features != None:
            selected_feature_indices = get_indices_of_features_from_csv(labelled_samples_path, selected_features)
        else:
            selected_feature_indices = np.arange(len(header))

        header = np.array(header)
        csv_train_writer.writerow(header[selected_feature_indices])
        csv_test_writer.writerow(header[selected_feature_indices])

        print('Count total number of samples ...')
        nr_samples = sum(1 for line in csv_r)  # count lines in csv
        csv_r.seek(0)
        csv_r.readline()

        if subsampling_fraction < 1:
            skip = sorted(random.sample(range(nr_samples), nr_samples - int(subsampling_fraction * nr_samples)))
            skip_idx = 0

        for nr, row in enumerate(
                tqdm(csv_reader, total=nr_samples, desc='Parsing the .csv and writing train and test-sets')):
            if subsampling_fraction < 1:
                if skip_idx < len(skip):
                    if nr == skip[skip_idx]:
                        skip_idx += 1
                        continue
                        # print('line {} skipped'.format(nr))

            if row[prot_idx] == '6' and row[syn_cnt_idx] == '0':
                tcp_zero_syn_cnt += 1
                continue

            row = np.array(row)
            if nr < train_fraction * nr_samples:
                csv_train_writer.writerow(row[selected_feature_indices])
            else:
                csv_test_writer.writerow(row[selected_feature_indices])

    print('{} ({}%) TCP-Flows skipped due to zero SYN count'.format(tcp_zero_syn_cnt, tcp_zero_syn_cnt * 100.0 / nr_samples))

    return csv_train_path, csv_test_path


def delete_zeroSYN_flows_from_csv(csv_path):
    """
    Function to delete TCP flows with Syn flag count = 0 from a .csv. The data will be written to the same directory and
    '-noZeroSYN.csv' will be appended to the original filename.

    """
    csv_out_path = os.path.splitext(csv_path)[0] + '-noZeroSYN.csv'
    tcp_zero_syn_cnt = 0
    nr_samples = 0

    with open(csv_path, 'r') as csv_r, open(csv_out_path, 'w') as csv_w:
        csv_reader = csv.reader(csv_r, delimiter=',', quotechar='#')
        csv_writer = csv.writer(csv_w, delimiter=',', quotechar='#')

        header = csv_reader.__next__()
        prot_idx = header.index('Protocol')
        syn_cnt_idx = header.index('SYN Flag Cnt')

        csv_writer.writerow(header)
        for nr, row in enumerate(tqdm(csv_reader, desc='Parsing the .csv')):
            if row[prot_idx] == '6' and row[syn_cnt_idx] == '0':
                tcp_zero_syn_cnt += 1
                nr_samples += 1
                continue

            csv_writer.writerow(row)
            nr_samples += 1

    print('{} ({}%) TCP-Flows skipped due to zero SYN count'.format(tcp_zero_syn_cnt, tcp_zero_syn_cnt * 100.0 / nr_samples))


def subsample_csv(csv_path, subsampling_fraction):
    """
    Function to subsample the rows of a .csv file

    """
    csv_out_path = os.path.splitext(csv_path)[0] + '-sub{}.csv'.format(subsampling_fraction)
    with open(csv_path, 'r') as csv_r, open(csv_out_path, 'w') as csv_w:
        print('Count total number of samples ...')
        nr_samples = sum(1 for line in csv_r)  # count lines in csv
        csv_r.seek(0)

        skip = sorted(random.sample(range(nr_samples), nr_samples - int(subsampling_fraction * nr_samples)))
        skip_idx = 0
        if skip[0] == 0:
            del skip[0] # don't skip the header

        for nr, row in enumerate(tqdm(csv_r, total=nr_samples, desc='Subsampling the .csv ...')):
            if skip_idx < len(skip):
                if nr == skip[skip_idx]:
                    skip_idx += 1
                    continue
            csv_w.write(row)

    print('Subsampled .csv written to {}.\n {} of {} samples subsampled ({}%)'.format(csv_out_path, nr_samples-len(skip), nr_samples, subsampling_fraction))


def cut_csv(csv_path, nr_samples):
    """
    Function to extract the first nr_samples samples from a .csv file. They will be stored to a new .csv in the same directory
    with the suffix '-<nr_samples>samples.csv' appended to the original filename


    """
    csv_out_path = os.path.splitext(csv_path)[0] + '-{}samples.csv'.format(nr_samples)
    with open(csv_path, 'r') as csv_r, open(csv_out_path, 'w') as csv_w:

        for nr, row in enumerate(tqdm(csv_r, total=nr_samples, desc='Subsampling the .csv ...')):
            if nr==nr_samples:
                break
            csv_w.write(row)


def merge_csv(csv_path_1, csv_path_2, outname, shuffle):
    """
    Function to merge two .csv files.

    :param csv_path_1:
    :param csv_path_2:
    :param outname: Name of the new .csv file
    :param shuffle: Set to true tu shuffle the data before saving to .csv
    """
    outpath = os.path.join(os.path.dirname(csv_path_1), outname)

    print('Reading .csv ...')
    df1 = pd.read_csv(csv_path_1, sep=',')
    df2 = pd.read_csv(csv_path_2, sep=',')

    df_out = pd.concat([df1, df2])
    if shuffle:
        df_out.sample(frac=1).reset_index(drop=True)

    print('Writing merged .csv to {} ...'.format(outpath))
    df_out.to_csv(outpath, sep=',')


def merge_csv_2(csv_path_1, csv_path_2, outname, shuffle):
    """
    Function to merge two .csv files. (Also suitable for big .csv files, as not the complete files have to be loaded into memory)

    :param csv_path_1:
    :param csv_path_2:
    :param outname: Name of the new .csv file
    :param shuffle: Set to true tu shuffle the data before saving to .csv
    """

    outpath = os.path.join(os.path.dirname(csv_path_1), outname)

    print('Merging to {} ...'.format(outpath))
    with open(outpath, 'w') as fp_w:
        with open(csv_path_1, 'r') as fp_r1:
            for line in fp_r1:
                fp_w.write(line)
        with open(csv_path_2, 'r') as fp_r2:
            fp_r2.readline() # skip header
            for line in fp_r2:
                fp_w.write(line)

    if shuffle:
        print('Shuffling the data ...')
        df = pd.read_csv(outpath, sep=',')
        df.sample(frac=1).reset_index(drop=True)
        # df.sample(frac=1)
        # df = df.iloc[np.random.permutation(len(df))]

        print('Writing shuffled .csv to {} ...'.format(outpath))
        df.to_csv(outpath, sep=',')


def remove_infinity_from_csv(csv_path):
    """
    Function to rempove rows containing 'Infinity' values from .csv.

    """
    out_path = os.path.splitext(csv_path)[0] + '-noInf.csv'

    with open(csv_path, 'r') as csv_r, open(out_path, 'w') as csv_w:
        csv_reader = csv.reader(csv_r, delimiter=',', quotechar='#', lineterminator='\n')
        csv_writer = csv.writer(csv_w, delimiter=',', quotechar='#', lineterminator='\n')

        header = csv_reader.__next__()
        header = [name.strip() for name in header]
        csv_writer.writerow(header)

        for row in csv_reader:
            if 'Infinity' in row:
                continue
            else:
                csv_writer.writerow(row)


def shuffle_csv(csv_path):
    """
    Function to shuffle a .csv

    """
    out_path = os.path.splitext(csv_path)[0] + '-shuffled.csv'
    with open(csv_path, 'r') as csv_r, open(out_path, 'w') as csv_w:
        header = csv_r.readline()

        csv_w.write(header)
        lines = csv_r.readlines()
        random.shuffle(lines)

        csv_w.writelines(lines)


### DATA LOADER ###

def load_dataset_basic(csv_data_path, train_fraction):
    """
    Basic function to load data from .csv into numpy arrays X, Y, where X contains the features and Y the corresponding labels

    :param csv_data_path: .csv file containing the data to be loaded
    :param train_fraction: Set to value < 1.0 to split the samples in csv_data_path into train and testsets
    :return: X_training, Y_training, X_test, Y_test
    """

    print('Loading dataset ...')
    train_data = pd.read_csv(csv_data_path, sep=',').values

    train_size = int(train_fraction * len(train_data))

    X_training = train_data[:train_size, :-1]
    Y_training = train_data[:train_size, -1]

    X_test = train_data[train_size:, :-1]
    Y_test = train_data[train_size:, -1]

    return X_training, Y_training, X_test, Y_test


def load_dataset(csv_data_path, train_fraction, selected_feature_names=None, balance=False, subsampling=None,
             standardize=False, categorical_feature_mapping=None, one_hot=False, pca_components=None):
    """
    Function to load data from .csv into numpy arrays X, Y, where X contains the features and Y the corresponding labels

    :param csv_data_path: .csv file containing the data to be loaded
    :param train_fraction: Set to value < 1.0 to split the samples in csv_data_path into train and testsets
    :param selected_feature_names: Names of the features/columns to be loaded from csv_data_path
    :param balance: Set True to balance the data. (#samples same for all classes)
    :param subsampling: To specify a fraction for subsampling. Set to False to keep all data.
    :param standardize: Set to True to standardize data, or privide a path to a .pickle file containing a stored standardizer
    :param categorical_feature_mapping: Dictionary to map categorical features to numerical values (see doc of function load_dataset())
    :param one_hot: If categorical features are present, set this parameter to True to enable one hot encoding
    :param pca_components: To enable principal component analysis: Set to nr of pca components to be used
    :return: X_train, Y_train, X_test, Y_test
    """
    print('Loading dataset ...')
    if selected_feature_names:
        print('... using feature selection')
        data = pd.read_csv(csv_data_path, sep=',', usecols=selected_feature_names)[selected_feature_names] # [selected_feature_names] to keep columns in specified order
    else:
        print('... using all features')
        data = pd.read_csv(csv_data_path, sep=',')

    train_size = int(train_fraction * len(data))

    if categorical_feature_mapping:
        data = map_categorical_features(data, categorical_feature_mapping, one_hot)
    else:
        data = data.values

    X_train = data[:train_size, :-1]
    Y_train = data[:train_size, -1]

    X_test = data[train_size:, :-1]
    Y_test = data[train_size:, -1]

    if standardize != False:
        if standardize == True:
            print('... Standardizing')
            scaler = preprocessing.StandardScaler().fit(X_train)
        elif '.pickle' in standardize:
            print('... Standardizing using {}'.format(standardize))
            with open(standardize, 'rb') as handle:
                scaler = pickle.load(handle)
        else:
            print('Invalid argument for standardizing data')
            return -1
        X_train = scaler.transform(X_train)
        if train_fraction < 1:
            # scaler = preprocessing.StandardScaler().fit(X_test)
            X_test = scaler.transform(X_test)

    if subsampling:
        X_train, Y_train = subsample(X_train, Y_train, subsampling)

    if balance:
        X_train, Y_train = balance_classes(X_train, Y_train)

    pca = None
    if pca_components:
        print('... PCA')
        # pca = PCA(.95)
        pca = PCA(n_components=pca_components)
        pca.fit(X_train)
        X_train = pca.transform(X_train)
        if train_fraction < 1:
            X_test = pca.transform(X_test)
        return X_train, Y_train, X_test, Y_test, pca

    else:
        return X_train, Y_train, X_test, Y_test


def load_dataset_seperate(csv_train_path, csv_test_path, merge=False, selected_feature_names=None, standardize=False,
                          balance=False, subsampling=None, swap_traintest=False, categorical_feature_mapping=None,
                          one_hot=False, pca_components=None):
    """
    Same function as load_dataset(), but train and test data is loaded from two seperate .csv files
    """

    print('Loading dataset ...')
    if selected_feature_names:
        print('... using feature selection: {} features'.format(len(selected_feature_names)-1))
        train_data = pd.read_csv(csv_train_path, sep=',', usecols=selected_feature_names)[selected_feature_names]
        test_data = pd.read_csv(csv_test_path, sep=',', usecols=selected_feature_names)[selected_feature_names]
    else:
        print('... using all features')
        train_data = pd.read_csv(csv_train_path, sep=',')
        test_data = pd.read_csv(csv_test_path, sep=',')

    if categorical_feature_mapping:
        train_data = map_categorical_features(train_data, categorical_feature_mapping, one_hot)
        test_data = map_categorical_features(test_data, categorical_feature_mapping, one_hot)
    else:
        train_data = train_data.values
        test_data = test_data.values

    X_train = train_data[:, :-1]
    X_test = test_data[:, :-1]
    Y_train = train_data[:, -1]
    Y_test = test_data[:, -1]

    if merge:
        X = np.concatenate((X_train, X_test), axis=0)
        Y = np.concatenate((Y_train, Y_test), axis=0)

        if balance:
            X, Y = balance_classes(X, Y)

        if subsampling:
            X, Y = subsample(X, Y, subsampling)

        if standardize:
            print('... Standardizing')
            scaler = preprocessing.StandardScaler().fit(X)
            X = scaler.transform(X)

        return X, Y
    else:
        if swap_traintest:
            print('Swapping train and test-set')
            X_train, X_test = X_test, X_train
            Y_train, Y_test = Y_test, Y_train

        if standardize:
            scaler = preprocessing.StandardScaler().fit(X_train)
            X_train = scaler.transform(X_train)
            # scaler = preprocessing.StandardScaler().fit(X_test)
            X_test = scaler.transform(X_test)

        if subsampling:
            X_train, Y_train = subsample(X_train, Y_train, subsampling)

        if balance:
            X_train, Y_train = balance_classes(X_train, Y_train)

        if pca_components:
            print('... PCA')
            pca = PCA(n_components=pca_components)
            pca.fit(X_train)
            X_train = pca.transform(X_train)
            X_test = pca.transform(X_test)

        return X_train, Y_train, X_test, Y_test


def save_scaler(csv_data_path, output_filename, selected_feature_names=None):
    """
    Function to save a standardscaler fitted on csv_data_path to .pickle file

    """
    print('Loading dataset ...')
    if selected_feature_names:
        print('... using feature selection')
        data = pd.read_csv(csv_data_path, sep=',', usecols=selected_feature_names)[selected_feature_names] # [selected_feature_names] to keep columns in specified order
    else:
        print('... using all features')
        data = pd.read_csv(csv_data_path, sep=',')


    X = data.values[:, :-1]

    print('... Standardizing')
    scaler = preprocessing.StandardScaler().fit(X)
    scaler.transform(X)

    with open(output_filename+'.pickle', 'wb') as handle:
        pickle.dump(scaler, handle)


def map_categorical_features(df, categorical_feature_mapping, one_hot):
    """
    Maps categorical features to numerical values.

    :param df: pandas datafreame containing the data
    :param categorical_feature_mapping: Dictionary to map categorical features to numerical values (see doc of function load_dataset())
    :param one_hot: Set to True for one hot encoding
    :return: numpy array of the numerical data
    """
    def get_nr_values_from_cat_mapping(categorical_feature_mapping):
        nr_values = []
        for cat, values in categorical_feature_mapping.items():
            nr_values.append(len(values.keys()))
        return nr_values

    # maps feature name strings to integers (sklearns OneHotEncoder doesn't accept strings as input)
    print('... Mapping categorical features to integers')
    df.replace(categorical_feature_mapping, inplace=True)

    if one_hot:
        print('... One hot encoding of categorical features')
        cat_indices = get_feature_indices_by_name(list(categorical_feature_mapping.keys()), list(df.columns.values))
        onehotencoder = OneHotEncoder(categorical_features=cat_indices, n_values=get_nr_values_from_cat_mapping(categorical_feature_mapping))
        return onehotencoder.fit_transform(df.values).toarray()
    else:
        return df.values


def subsample(X, Y, subsampling_fraction, seed=None):
    """
    Function for subsampling of data arrays (X: features, Y: labels)

    """
    print('... Subsampling with factor {}'.format(subsampling_fraction))
    if seed:
        np.random.seed(seed)
    subsampling_idx = np.random.choice(len(Y), int(subsampling_fraction * len(Y)))
    print('{} out of {} samples selected'.format(len(subsampling_idx), len(X)))
    return X[subsampling_idx], Y[subsampling_idx]


def balance_classes(X, Y):
    """
    Function to balance a dataset. (#samples same for all classes)

    :return: balanced X, Y
    """
    print('... Balancing class distributions')
    mal_indices, = np.where(Y == 1)
    normal_indices, = np.where(Y == 0)
    subsampling_idx = np.random.choice(len(normal_indices), len(mal_indices))
    normal_indices = normal_indices[subsampling_idx]
    X = np.concatenate((X[normal_indices], X[mal_indices]), axis=0)
    Y = np.concatenate((Y[normal_indices], Y[mal_indices]), axis=0)
    random_permutation = np.random.permutation(np.arange(len(Y)))
    X = X[random_permutation]
    Y = Y[random_permutation]

    return X, Y


def get_indices_of_features_from_csv(csv_path, feature_names):
    """
    :return: List of the column numbers of the specified feature_names in csv_path
    """
    feature_indices = []
    with open(csv_path, 'r') as csv_r:
        csv_reader = csv.reader(csv_r, delimiter=',', quotechar='#')
        header = csv_reader.__next__()

    for nr, feature in enumerate(feature_names):
        feature_indices.append(header.index(feature_names[nr]))

    return feature_indices


def get_feature_indices_by_name(selected_feature_names, all_feature_names):
    """
    :return: The indices of the elements specified in selected_feature_names in the list all_feature_names
    """
    selected_feature_indices = []
    for feature in selected_feature_names:
        selected_feature_indices.append(all_feature_names.index(feature))
    return selected_feature_indices


def eliminate_features(X, eliminate_feature_names, all_feature_names):
    """
    Function to remove some features/columns from X

    :param eliminate_feature_names: Names of the features/columns to be removed
    :param all_feature_names: Names of all the features/columns of X
    :return: X without the removed columns
    """
    eliminate_feature_indices = get_feature_indices_by_name(eliminate_feature_names, all_feature_names)
    all_feature_indices = np.arange(X.shape[1])

    remaining_indices = list(set(all_feature_indices) - set(eliminate_feature_indices))
    remaining_names = np.array(all_feature_names)[remaining_indices]

    return X[:, remaining_indices], remaining_names


def eliminate_features_by_index(X, eliminate_feature_indices):
    """
    Function to remove some features/columns from X

    :param eliminate_feature_indices: Indices of te columns to be removed
    :return: X without the removed columns
    """
    all_feature_indices = np.arange(X.shape[1])

    remaining_indices = list(set(all_feature_indices) - set(eliminate_feature_indices))

    return X[:, remaining_indices]


def get_remaining_indices(eliminate_feature_names, all_feature_names):
    """
    :return: List of the feature indices that remain after removing
    """
    eliminate_feature_indices = get_feature_indices_by_name(eliminate_feature_names, all_feature_names)
    all_feature_indices = np.arange(len(all_feature_names))

    remaining_indices = list(set(all_feature_indices) - set(eliminate_feature_indices))
    return remaining_indices


### STATISTICS ###

def get_labelling_statistics(labelled_samples_path, skip_no_SYN):
    """
    Function to generate some statistics of data stored in .csv:
    - nr & % of malicious samples in the data
    - nr & % of tcp samples with zero SYN count

    :param labelled_samples_path:
    :param skip_no_SYN:
    """
    print('Processing {} ...'.format(labelled_samples_path))

    nr_samples = 0
    nr_mal_samples = 0
    tcp_zero_syn_cnt = 0

    with open(labelled_samples_path, 'r') as csv_r:
        csv_reader = csv.reader(csv_r, delimiter=',', quotechar='#')
        header = csv_reader.__next__()
        label_idx = header.index('Label')
        if skip_no_SYN:
            syn_cnt_idx = header.index('SYN Flag Cnt')
            prot_idx = header.index('Protocol')

        for row in csv_reader:
            if skip_no_SYN:
                if row[prot_idx] == '6' and row[syn_cnt_idx] == '0':
                    tcp_zero_syn_cnt += 1
                    continue

            if row[label_idx] == '1':
                nr_mal_samples += 1
            nr_samples += 1

    print('Data contains {}% malicious samples ({} out of {})'.format(nr_mal_samples * 100 / nr_samples, nr_mal_samples,
                                                                      nr_samples))
    if skip_no_SYN:
        print('{} ({}%) TCP-Flows skipped due to zero SYN count'.format(tcp_zero_syn_cnt,
                                                                        tcp_zero_syn_cnt * 100.0 / nr_samples))


### PLOTTING ###

def plot_subsampling_precision_recall_csv(csv_path, filename):
    """
    Function to generate precision-recall plots

    :param csv_path: Precision, Recall values in .csv format
    """
    df = pd.read_csv(csv_path, sep=',')
    fractions = df['fraction'].values
    precisions = df['precision'].values
    recalls = df['recall'].values

    nr_samples = len(fractions)

    ax = plt.subplot(111)

    ax.plot(fractions, precisions, label='Precision')
    ax.plot(fractions, recalls, label='Recall')
    plt.xlabel('Packet loss')
    ax.legend()
    # plt.title('Scores with packet loss')
    plt.xticks(fractions)

    plt.savefig('./figures/{}.png'.format(filename))
    plt.savefig('./figures/{}.eps'.format(filename), format='eps')
    plt.show()


def plot_feature_distribution(csv_path, feature_name, binwidth):
    """
    To generate bar plots of feature value distributions

    :param csv_path: Data path
    :param feature_name: Name of the feature to be visualized
    :param binwidth: With of the barplot bins
    """
    import seaborn as sns
    sns.set(color_codes=True)
    np.random.seed(sum(map(ord, "distributions")))

    plt.show()
    X, Y, _, _ = load_dataset(csv_path, 1, selected_feature_names=feature_name, standardize=False)

    normal_samples = X[np.where(Y==0)]
    malicious_samples = X[np.where(Y==1)]

    # sns_plot = sns.distplot(X, kde=False, rug=True)
    # sns_plot.savefig("sns.png");

    bins = range(int(min(X)), int(max(X))+binwidth, binwidth)
    # bins = range(10)
    fig, axes = plt.subplots(1, 2, sharey=True)

    # Set the ticks and ticklabels for all axes
    plt.setp(axes, xticks=bins)

    axes[0].hist(normal_samples, color='blue', bins=bins)
    # axes[0].set_title('SYN count (normal)')
    axes[1].hist(malicious_samples, color='red', bins=bins)
    # axes[1].set_title('SYN count (malicious)')
    # plt.hist(X, color='blue', edgecolor='black', bins=range(min(X), max(X) + binwidth, binwidth))
    # plt.hist(X, color='blue')

    fig.tight_layout()

    stamp = str(int(datetime.now().timestamp()))
    plt.savefig('./figures/hist-{}.png'.format(stamp))
    plt.savefig('./figures/hist-{}.eps'.format(stamp), format='eps')
    plt.show()


def multi_bar_plot(values_list, method_names, tick_names, fontsize=5, logscale=False, save=False):
    """
    General function to generate barplots with multiple bars per bin

    """

    # data to plot
    nr_methods = len(values_list)
    nr_scores = len(values_list[0])
    bar_width = 0.8 / nr_methods

    for nr, values in enumerate(values_list):
        plt.bar(np.arange(nr_scores) + nr * bar_width, values, bar_width, align='center', label=method_names[nr],
                log=logscale)

    if logscale:
        plt.yscale('log')

    plt.xticks(range(nr_scores), tick_names, rotation='vertical', fontsize=fontsize)
    plt.tight_layout()
    plt.legend()
    if save:
        stamp = str(int(datetime.now().timestamp()))
        plt.savefig('./figures/barplot-{}.png'.format(stamp))
        plt.savefig('./figures/barplot-{}.eps'.format(stamp), format='eps')
    plt.show()


def multi_bar_plot_by_dict(scores_dict, fontsize=10, logscale=False, save=False):
    """
    General function to generate barplots with multiple bars per bin

    """
    # data to plot
    nr_methods = len(scores_dict)
    bar_width = 0.8 / nr_methods

    last_keys = None
    i = 0
    for method, score_dict in scores_dict.items():
        keys = score_dict.keys()
        nr_scores = len(score_dict)
        # check if the keys of all dictionaries are in iterated in same order...
        if last_keys != None and last_keys != keys:
            print('Key-order mismatch')
            return
        plt.bar(np.arange(nr_scores) + i * bar_width, list(score_dict.values()), bar_width, align='center',
                label=method, log=logscale)
        last_keys = keys
        i += 1
    if logscale:
        plt.yscale('log')

    plt.xticks(range(nr_scores), list(keys), rotation='vertical', fontsize=fontsize)
    plt.tight_layout()
    plt.legend()
    if save:
        stamp = str(int(datetime.now().timestamp()))
        plt.savefig('./figures/barplot-{}.png'.format(stamp))
        plt.savefig('./figures/barplot-{}.eps'.format(stamp), format='eps')
    plt.show()


def plot_tf_pr_summary_csv(csv_path, save):
    """
    Function to print precision and recall values extracted from tensorflow summaries

    """

    precision_files = []
    recall_files = []

    for f in os.listdir(csv_path):
        if f.endswith(".csv"):
            if 'precision' in f:
                precision_files.append(f)
            elif 'recall' in f:
                recall_files.append(f)

    precision_files = sorted(precision_files, key=lambda x: int(x.split('_')[1]))
    recall_files = sorted(recall_files, key=lambda x: int(x.split('_')[1]))

    f, axarr = plt.subplots(2, sharex=True)
    labels = ['']

    for prec_f, rec_f in zip(precision_files, recall_files):
        axarr[0].set_title('Precision')
        precisions = pd.read_csv(os.path.join(csv_path, prec_f), sep=',').values[:, 2]
        axarr[0].plot(precisions)
        axarr[0].set_ylim([0, 1])

        axarr[1].set_title('Recall')
        recalls = pd.read_csv(os.path.join(csv_path, rec_f), sep=',').values[:, 2]
        axarr[1].plot(recalls)
        axarr[1].set_ylim([0, 1])
    plt.show()


    stamp = str(int(datetime.now().timestamp()))
    if save:
        plt.savefig('./figures/pre_rec-{}.png'.format(stamp))
        plt.savefig('./figures/pre_rec-{}.eps'.format(stamp), format='eps')


def plot_manifold(x, y, method_name, y_to_name_mapping=None, cluster_centers=None, subsampling=None, colors=None, suffix=''):
    """
    Function for visualizations of highdimensional data.

    :param x: Data samples to be visualized
    :param y: Labels of the data samples
    :param method_name: Name of the visualization method to be used. Available: 'PCA', 'TSNE', 'LLE', 'SE', 'ALL' (to run all methods)
    :param y_to_name_mapping: To map the label values provided in y to other values / names
    :param cluster_centers: Cluster centers can be passed here, and will be represented by a red cross in the plot
    :param subsampling: Set a factor to subsample the data before the visualization
    :param colors: Specify a set of colors to be used for the differnt classes
    :param suffix: Suffix for the filename of the saved plot
    """
    methods = {'PCA': PCA(n_components=2), 'TSNE': TSNE(n_components=2, init='random', random_state=0, perplexity=70),
               'LLE': LocallyLinearEmbedding(30, n_components=2, method='standard'), 'SE': SpectralEmbedding(n_components=2, random_state=0, eigen_solver="arpack")}
    all = False

    if method_name == 'PCA':
        method = methods['PCA']
    elif method_name == 'TSNE':
        method = methods['TSNE']
    elif method_name == 'LLE':
        method = methods['LLE']
    elif method_name == 'SE':
        method = methods['SE']
    elif method_name == 'ALL':
        all = True
    else:
        print('{} is not a valid method. Valid options are : PCA, TSNE, LLE, SE, ALL'.format(method_name))
        return

    if subsampling != None:
        x, y = subsample(x, y, subsampling)

    # colors = ['r', 'b']
    target_ids = list(set(y))
    nr_labels = len(target_ids)
    labels = []
    if y_to_name_mapping != None:
        for id in target_ids:
            labels.append(y_to_name_mapping[id])
        target_ids = [id for label, id in sorted(zip(labels, target_ids))]
        labels = [label for label, id in sorted(zip(labels, target_ids))]
    else:
        labels = [str(nr) for nr in target_ids]

    if colors == None or nr_labels-1 > 19:
        colors = list(cm.rainbow(np.linspace(0,1,nr_labels-1)))
    else:
        colors = colors[:nr_labels-1]
    colors = ['black'] + colors

    if all:
        fig, axs = plt.subplots(2,2)
        axs = axs.flatten()
        nr = 0
        for name, method in methods.items():
            x_2d = method.fit_transform(x)
            for i, label, c in zip(target_ids, labels, colors):
                axs[nr].scatter(x_2d[y == i, 0], x_2d[y == i, 1], label=label, marker="o", edgecolors=c, facecolors='none')
            axs[nr].set_title(name)
            nr += 1
        axs[nr-1].legend()
    else:
        if cluster_centers is not None:
            nr_clusters = len(cluster_centers)
            x = np.concatenate((x, cluster_centers))
            x_2d = method.fit_transform(x)
            centers_2d = x_2d[-nr_clusters:]
            x_2d = x_2d[:-nr_clusters]

        else:
            x_2d = method.fit_transform(x)

        fig = plt.figure(figsize=(8,5))
        ax = plt.subplot(111)

        for i, label, c in zip(target_ids, labels, colors):
            ax.scatter(x_2d[y == i, 0], x_2d[y == i, 1], label=label, marker="o", edgecolors=c, facecolors='none')

        if cluster_centers is not None:
            ax.scatter(centers_2d[:, 0], centers_2d[:, 1], label='cluster center', marker="x", color='r')

        box = ax.get_position()
        # shrink the current plot's width, and put the legend entirely outside the axis of the figure
        ax.set_position([box.x0, box.y0, box.width * 0.7, box.height])
        ax.legend(loc='center left', bbox_to_anchor = (1.0, 0.5))

    # plt.tight_layout()
    stamp = str(int(datetime.now().timestamp()))
    plt.savefig("figures/manifold-{}-{}.png".format(stamp, suffix))
    plt.show()


def plot_tsne_with_different_perplexities(x, y, perplexities,  y_to_name_mapping=None, subplots=False):
    """
    Function to generate multiple TSNE plots for different perlplexity values.

    :param x: Data samples to be visualized
    :param y: Labels of the data samples
    :param perplexities: List of the perplexity values
    :param y_to_name_mapping: To map the label values provided in y to other values / names
    :param subplots: If True generate one figure with subplots. If False generate seperate figure for each perplexity
    """
    target_ids = list(set(y))
    nr_labels = len(target_ids)
    labels = []
    if y_to_name_mapping != None:
        for id in target_ids:
            labels.append(y_to_name_mapping[id])
    else:
        labels = [str(nr) for nr in target_ids]

    colors = list(cm.rainbow(np.linspace(0, 1, nr_labels - 1)))
    colors = ['black'] + colors

    if subplots:
        fig, axs = plt.subplots(1, len(perplexities))
        axs.flatten()
        for nr, perplexity in enumerate(perplexities):
            tsne = TSNE(n_components=2, init='random', random_state=0, perplexity=perplexity)

            x_2d = tsne.fit_transform(x)

            for i, label, c in zip(target_ids, labels, colors):
                axs[nr].scatter(x_2d[y == i, 0], x_2d[y == i, 1], label=label, marker="o", edgecolors=c, facecolors='none')
            axs[nr].set_title('p={}'.format(perplexity))

        # axs[-1].legend()
        axs[-1].legend(loc='center left', bbox_to_anchor=(1.0, 0.5))

        plt.savefig("figures/tsne_perplexities.png")
        plt.show()
    else:
        for nr, perplexity in enumerate(perplexities):
            tsne = TSNE(n_components=2, init='random', random_state=0, perplexity=perplexity)

            x_2d = tsne.fit_transform(x)

            fig = plt.figure(figsize=(8, 5))
            ax = plt.subplot(111)

            for i, label, c in zip(target_ids, labels, colors):
                ax.scatter(x_2d[y == i, 0], x_2d[y == i, 1], label=label, marker="o", edgecolors=c, facecolors='none')

            ax.set_title('p={}'.format(perplexity))
            box = ax.get_position()
            # shrink the current plot's width, and put the legend entirely outside the axis of the figure
            ax.set_position([box.x0, box.y0, box.width * 0.7, box.height])
            ax.legend(loc='center left', bbox_to_anchor=(1.0, 0.5))

            plt.savefig("figures/tsne_perplexities-p{}.png".format(perplexity))
            plt.show()


def visualize_random_forest_decision_trees(model_path, out_directory):
    """
    Funcion to visualize the desicion trees of a random forest model.
    Caution: Check first the number of nodes of the RF model

    :param model_path: Path of the random forest model to be viszualized
    :param out_directory: Directory where the different tree visualizations are stored
    """
    from sklearn import tree
    forest = load_model(model_path)
    i_tree = 0
    for tree_in_forest in forest.estimators_:
        with open(os.path.join(out_directory, 'tree_' + str(i_tree) + '.dot'), 'w') as my_file:
            my_file = tree.export_graphviz(tree_in_forest, out_file=my_file)
        i_tree = i_tree + 1


def get_rf_tree_nodeCnt_and_depth(model_path):
    """
    Get the node count and the depth of all decision trees of a random forest model

    :param model_path: Path of the random forest model
    """
    from sklearn import tree
    forest = load_model(model_path)
    i_tree = 0
    for nr, tree_in_forest in enumerate(forest.estimators_):
        print('Tree nr. {} (Depth, Node Count):  {}, {}'.format(nr, tree_in_forest.tree_.max_depth, tree_in_forest.tree_.node_count))
        i_tree = i_tree + 1